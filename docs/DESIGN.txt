This document describes the current vision for the high-level architecture of JetStream.


DEFINITIONS:

//SS: I don't think "queries are never standing", since a streaming query may continuously deliver.
// Perhaps all we need to say is that queries pull results from some hypercube (at any pace).
A query is a request by the user for a particular fact about a system. Queries are never standing. All query results must be present in some hypercube.

A hypercube stores multi-dimensional data about a system, typically in aggregate form. Hypercubes are the basic unit for representing data.

A computation is the process by which the system materializes data in hypercubes. A computation can be either standing or one-shot.

A task is the smallest unit of a computation that JetStream will explicitly place.

A source is a task that emits hypercubes.

An operator transforms one or more source hypercubes into a destination hypercube. Operators run on nodes.

A node is a computer on which JetStream can place sources or operators.

//SS: Need to define a "worker". Is it a process running on a node, or a thread? Should the coordinator 
//be concerned about workers, or just nodes?
A worker is the finest execution environment running on a node. A node runs one or more workers; a worker runs one or more tasks. 



SEMANTICS:

//SS: Since a table is just a 2-cube, we can make hypercubes the universal unit of data.
JetStream exists to maintain views of changing data. Data will be structured as tables or hypercubes (THIS IS VAGUE SO FAR.)

Each hypercube exists on a particular node.

Nodes may periodically checkpoint the hypercubes. These can be stored on either local or shared storage. Durability will be configured by the user.


RATIONALE:
Doing straight-ahead streaming of tuples seems to have miserable difficulties with failure recovery. Hypercubes make the state and buffering explicit. They also make time explicit.

Because time is part of the explicit state, the code to modify hypercubes and handle failures will no longer need to reason about ordering and time, greatly simplifying failure recovery. We may be able to avoid having any explicit recovery code -- just pour new data down the pipe and it'll get aggregated.


DIFFICULTIES:
Suppose we have a source, S1. and it's currently sending data to aggregation node A. And then A fails, and S1 fails over to aggregator B. How do we make sure that S1's input is counted exactly once?  Perhaps it's okay to keep linear state for "which nodes were included in this hypercube"?

Should CubeIDs include version numbers?  What happens if we have a series of cubes with a schema change


POSSIBLE API:

TYPES:
TaskID -- ID of a task. Currently 32-bit computation ID and 32-bit task ID
TaskMeta -- Information about a task. Includes a TaskID

CubeID -- the ID of a data cube. WHAT DO THESE LOOK LIKE?
DataCube -- A chunk o' data. 
DataCubeMeta -- Metadata for a cube. Includes schema.


NodeID -- ID of a worker node. Currently IP addr + portno.
//WorkerID -- ID of a worker. WHAT DO THEY LOOK LIKE?


API itself has moved to the .proto files.