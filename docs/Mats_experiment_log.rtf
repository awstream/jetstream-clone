{\rtf1\ansi\ansicpg1252\cocoartf1038\cocoasubrtf360
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset0 Tahoma;}
{\colortbl;\red255\green255\blue255;\red7\green21\blue105;}
\margl1440\margr1440\vieww25100\viewh14200\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\ql\qnatural\pardirnatural

\f0\fs24 \cf0 For processing a 765M file (apache log from adam)\
\
cp takes ~0.7s disk-to-disk, it probably lies\
dd takes 3-6s (130-230 MB/s) ram-to-disk, disk-to-same-disk, disk-to-other-disk, 1.6s (480 MB/s) ram-to-ram\
\
\
No-parsing: \
	-Line-by-line output\
		- ostream\
			12s w/newlines\
		- low level fd (open, write , close) no fsync\
			12s(w/o newlines, disk-to-disk)\
			10.9s(w/o newlines, ram-to-disk)\
			20s (w newlines\
			4.5s (w/o newlines, ram-to-ram)\
			4.5s (w/o newlines, disk-to-ram)\
		- low level fd (open, write , close) with fsync\
			did about 19Megabytes in 2 Hours\
with parsing\
		-to file\
			85s (disk-to-disk, old parsing using boost tokenizer)\
			38s (disk-to-disk, ram-to-disk)\
			29s disk-to-ram\
\
to db\
    myisam\
          --spin--\
	-17:46 m - with procedure call, old parse\
            -16:51 m - with procedure call, new parse\
	-14 m - no procedure call\
	-16 m with procedure call\
	-3.34 min with batching at b=100\
	-3.25 min with b=1000\
            ---ssd---\
	3.35 min win b=100\
     memory (set max_heap_table_size=1024*1024*2000;)\
	1.29 with b=100 source =ram\
	1.27 with b=100 source = disk\
     Innodb \
	--spin--\
	very slow\
	--ssd--\
	2:49 min with b=100\
\
	\
\
\
For processing 50Mb file\
to db (488,090 rows):\
--spin disk--\
	new parse:\
	with procedure call, no batch, time conversion in db - 59s, \
	without procedure call, no batch, time conversion in db - 47.5, \
	with procedure call, no batch, time conversion in c - 63 s\
	without procedure call, no batch, time conversion in c - 47.5s\
\
	with pc, no batch, time in db, old parse 60 s\
\
	conclusion: calling a stored procedure is expensive, strtotime isn't; hard to get consistent numbers for cost of strtotime;	\
\
	with batching (b is batch parameter) batching done with update on duplicate key clause, no procedure call:\
	b=10 - 21s\
	b=100 - 12s (same for from disk and from ram)\
	b=1000 - 11s \
\
\
engine=memory, b=100, no procedure call, time conversion in db, new parse - 5.6s\
\
---ssd---\
12s, engine = myisam, b=100, \
\
\
\
\
\
----spin disk----\
engine=innodb, b=100, no procedure call, time conversion in db, new parse - \
# w delete, which takes time in innodb\
(3.25m -global innodb_flush_log_at_trx_commit = 1)\
(42s -global innodb_flush_log_at_trx_commit = 0)\
(47s -global innodb_flush_log_at_trx_commit = 2)\
---ssd----\
engine=innodb, b=100, no procedure call, time conversion in db, new parse, from ram - \
# w/o delete, which takes time in innodb\
(10s -global innodb_flush_log_at_trx_commit = 1)\
(9.2s -global innodb_flush_log_at_trx_commit = 0)\
(9.3s -global innodb_flush_log_at_trx_commit = 2)\
\
\
My version of db events (full file)\
33 min w old parsing, procedure call, one at a time, myIsam <- not true looks like\
> 33min innodb engine\
16 min w new parsing, procedure call, one at a time, myIsam\
14 min w new parsing, no procedure call, one at a time, myIsam\
3.25 min w new parsing, no procedure call, batch, myIsam\
1.29min w new parsing, no procedure call, batch, memory\
\
--------------\
ssd setup\
sudo mount -o noatime,nobarrier /dev/sdc1 /disk/ssd\
echo noop > /sys/block/sdc/queue/scheduler\
sudo stop mysql\
sudo cp -R -p /var/lib/mysql /disk/ssd/mysql\
edit /etc/mysql/my.cnf     change matador\
edit 
\f1 /etc/apparmor.d/usr.sbin.mysqld Copy the lines beginning with "/var/lib/mysql", comment out the originals with hash marks ("#"), and paste the lines below the originals.\
\
 sudo /etc/init.d/apparmor reload\

\f0 --------------------\
ram mount\
sudo mount -t tmpfs -o size=2g tmpfs /disk/ram/\
\
-----------------------------------------------------------------------------------\
conclusions:\
\
ssd vs spin:\
makes a big difference for innodb, not for myisam\
--------------------------------------------------------------------------------------------------------\
\
bin/controller.sh\
jsnoded -C ../config/local.conf --start\
bin/pyrunner.sh src/python/jetstream/examples/topk_perftest.py -n -o ~/perftest.log\
--------------------------------------------------------------------------------------------------------\
\
rtt across local = 1ms\
rtt wan = 80ms\
\
Note the following is just from throuput, may not be accurate:\
from our data, latency of db w/o batch = 97 us\
{\field{\*\fldinst{HYPERLINK "http://www.mysqlperformanceblog.com/2012/05/16/benchmarking-single-row-insert-performance-on-amazon-ec2/"}}{\fldrslt http://www.mysqlperformanceblog.com/2012/05/16/benchmarking-single-row-insert-performance-on-amazon-ec2/}}\
shows insert rate of single-row inserts to be ~25k rows/sec = 40us on avg\
note batch can get much higher numbers (~200-300k/sec).\
{\field{\*\fldinst{HYPERLINK "http://www.slideshare.net/tazija/evaluating-nosql-performance-time-for-benchmarking"}}{\fldrslt http://www.slideshare.net/tazija/evaluating-nosql-performance-time-for-benchmarking}} - show much higher latency numbers\
\
\
\
hd = 
\fs26 Average seek time ranges from 3\'a0{\field{\*\fldinst{HYPERLINK "http://en.wikipedia.org/wiki/Millisecond"}}{\fldrslt \cf2 ms}}{\field{\*\fldinst{HYPERLINK "http://en.wikipedia.org/wiki/Hard_disk_drive_performance_characteristics#cite_note-WD-VRaptor_Specs-12"}}{\fldrslt 
\fs20 \cf2 [12]}} for high-end server drives, to 15\'a0ms for mobile drives, with the most common mobile drives at about 12\'a0ms{\field{\*\fldinst{HYPERLINK "http://en.wikipedia.org/wiki/Hard_disk_drive_performance_characteristics#cite_note-WD-Scorpio_Blue-specs-13"}}{\fldrslt 
\fs20 \cf2 [13]}} and the most common desktop drives typically being around 9\'a0ms.\
-----------------------------------------------------------------------------\
./bin/controller.sh -C config/vicci_mn.conf \
bin/pyrunner.sh src/python/jetstream/examples/topk_perftest.py -n -o ~/perftest.log --controller 128.112.171.38:3456\
./jsnoded -C ../config/princeton_mn.conf --start -d 128.112.171.44:0\
-----------------------------------------------------------------------------\
\
Thought on intro; this is how I would frame the work:\
\
We increasingly have data being generated in the wide area. Examples: CDN logs, routing data, distributed service logs (Facebook, gmail). And we have an increasing need to analyze this data for business and operational purposes. Interestingly, on the operational side we have an increasing need for rapid response to changes in system state such as load. Examples: CDN load balancing, Google's use of SDN for network control, rapid DoS response, BGP blackhole detections. In addition, network bandwidth remains expensive especially on a global scale [cite cross-oceanic cable $$]. So, we want to avoid backhaul as much as possible. \
\
So, what we is needed is a system that can analyze data originating in the wide area in two ways:\
1) Standing queries for operational/dynamic control \
2) Historical queries for business intelligence, billing, trend analysis. This is needed because we don't always know which queries of data we need when the data is first generated.\
\
Right now we have three major available solutions:\
1) Databases.\
2) Streaming systems for low latency operation control\
3) Map-Reduce type analysis\
\
\
If we place DBs at the edges, then each query needs to go back to all the multitude of sources to get results for new queries. There are 2 problems with this design: (i) sources will fail often and then we loose all their data for historical queries and (ii) we observe that oftentimes subsequent queries reuse data from older queries and thus it would be nice if subsequent queries could re-use previous answers. We propose using in-network storage-aggregators where data can be aggregated with high fidelity within the local area and be made available to subsequent queries. These boxes could be made more fault tolerant and new queries could be fulfilled by contacting these storage-aggregators instead of going back to the sources. The alternative of having one central database is not enticing either because of the cost of backhaul (which is expensive\'85). \
\
Streaming systems are not sufficient because we have no ability to perform historical queries. Further, if we try the naive approach of combining storage with a streaming system by putting databases at the sources, we get the same issues outlined when talking about databases.  Placing storage at the sink of a streaming query does not work either because queries are then constrained by the data that is sent in the initial streaming query, which either has to send a lot of data or severely constrain the set of possible queries.   \
\
Map-Reduce type queries does not work because of the synchronization barriers inherent to this approach. In particular the reduce phase has to wait for all mappers to complete before outputting the result. This is a problem in the wide-area since it forces the entire query to wait for the slowest node or datacenter, creating high latency for streaming queries. \
\
We propose Jetstream, a system that supports both streaming and historical queries in the wide-area. In  our design, OLAP style data-cubes are connected together by streaming data-processing operators. When data comes in it is stored locally at the source and is also sent along to fulfill any standing queries. Standing queries are used to build up in-network storage-aggregator data cubes that can be subsequently queried for more efficient historical querying and are also used to fulfill low-latency operation queries for dynamic control.  \
\
Our data cubes contains a set of dimensions and a set of aggregates. \
 \
\

\fs24 \
\
}